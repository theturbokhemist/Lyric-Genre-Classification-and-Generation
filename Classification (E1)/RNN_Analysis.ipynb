{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Instructions to run code**\n",
        "\n",
        "This is a colab notebook and thus requires some specific steps to run.\n",
        "\n",
        "1.) Upload the csv from this dataset (https://www.kaggle.com/datasets/novanglus/music-lyrics-by-genre) to your google drive.\n",
        "\n",
        "2.) Mount the drive and move this csv to the local colab directory. I show this in some of the code below.\n",
        "\n",
        "3.) The rest of the code should run smoothly. We recommend using a GPU or you may have memory issues or it may run extremely slowly. When you get to the WandB section at the end you will need to and generate an API key from their website."
      ],
      "metadata": {
        "id": "AKPcGUvz2lvc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sPNGmid6eMOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da0e5591-2fd7-4f45-ad63-72ecc5cf3a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#Importing libraries\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "import nltk\n",
        "import math\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing #binarizing the labels\n",
        "\n",
        "import time \n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GPU type\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xpazP_deQge",
        "outputId": "3410da21-cc95-4a2c-f6ab-4b8bf42896fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 16 23:23:11 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8    12W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check available RAM\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oMFLsG0IB0G",
        "outputId": "a1d8fc08-24aa-44d9-e596-4d4c734fb389"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Confirm that GPU is available\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBAcTCsVhDk-",
        "outputId": "33bab7c1-1e54-4846-ad61-c2deb6ee3fce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIDTMtiPgr54",
        "outputId": "7695cf32-0899-4aff-f4ec-f4631932d78b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Move File from Drive to Colab\n",
        "!cp \"/content/gdrive/MyDrive/NLP_Project_Data/df_lyrics.csv\" /content  "
      ],
      "metadata": {
        "id": "mwT0CBcUeQi2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('df_lyrics.csv')[['Lyrics', 'Genre']]\n",
        "\n",
        "#df.columns = ['TEXT_COLUMN_NAME', 'LABEL_COLUMN_NAME']\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gD2dj6LfeQBh",
        "outputId": "d57cc561-84a7-4c9c-b4fc-6a85edaa8c35"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              Lyrics Genre\n",
              "0   Oh, baby, baby Oh, baby, baby  Oh, baby, baby...   Pop\n",
              "1   Baby, can't you see I'm calling? A guy like y...   Pop\n",
              "2   You wanna? You wanna?  You want a hot body? Y...   Pop\n",
              "3   Mmm, yeah Yeah, yeah, yeah, yeah, yeah, yeah ...   Pop\n",
              "4   La, la, la, la, la-la, la, la La, la, la, la,...   Pop"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-959aebb3-f7f0-48c7-b89a-784f2c0f3c35\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Lyrics</th>\n",
              "      <th>Genre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Oh, baby, baby Oh, baby, baby  Oh, baby, baby...</td>\n",
              "      <td>Pop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Baby, can't you see I'm calling? A guy like y...</td>\n",
              "      <td>Pop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You wanna? You wanna?  You want a hot body? Y...</td>\n",
              "      <td>Pop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mmm, yeah Yeah, yeah, yeah, yeah, yeah, yeah ...</td>\n",
              "      <td>Pop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>La, la, la, la, la-la, la, la La, la, la, la,...</td>\n",
              "      <td>Pop</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-959aebb3-f7f0-48c7-b89a-784f2c0f3c35')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-959aebb3-f7f0-48c7-b89a-784f2c0f3c35 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-959aebb3-f7f0-48c7-b89a-784f2c0f3c35');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we build a custom class that contains a variety of preprocessing methods for our data\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer \n",
        "stopWords = set(stopwords.words('english')) \n",
        "\n",
        "\n",
        "class PreProcess():\n",
        "  \"\"\"\n",
        "methods: decontraction, removing punctuations, lowercasing, \n",
        "removing non-alphanumeric characters, stemming\n",
        "  \"\"\"\n",
        "\n",
        "  def filterpun(self, text):\n",
        "\n",
        "    replace_by_space = re.compile('[<!./{}\\[\\]\\|@,;%#+_`]')\n",
        "    filtered = replace_by_space.sub('', text)\n",
        "    filtered = filtered.replace(\"'\", \"\")\n",
        "    filtered = filtered.replace('\"', \"\")\n",
        "    filtered = filtered.strip()\n",
        "    filtered = filtered.replace(\"\\n\",\" \")\n",
        "    return filtered\n",
        "\n",
        "\n",
        "  def lowercasing(self, text):\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "  def removespecial(self, text):\n",
        "    return re.sub(r'\\W+', ' ', text)\n",
        "\n",
        "\n",
        "  def stemming(self, sentence):\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    words = sentence.split()\n",
        "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "\n",
        "  def removeStop(self, text):\n",
        "\n",
        "    tokens = []\n",
        "\n",
        "    for token in text.split():\n",
        "\n",
        "      if token not in stopWords and token != '':\n",
        "\n",
        "        tokens.append(token)\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "#Initialize class\n",
        "preprocess = PreProcess()\n",
        "\n",
        "#function for applying all preprocessing how we want\n",
        "def preprocess_text(text, removeStopWords = False):\n",
        "    text = preprocess.filterpun(text)\n",
        "    text = preprocess.lowercasing(text)\n",
        "    #text = preprocess.removespecial(text)\n",
        "    #text = preprocess.stemming(text) #stemming lowers the accuracy too bad.\n",
        "    text = \" \".join(text.split()[:-1])\n",
        "    \n",
        "    if removeStopWords:\n",
        "      text = preprocess.removeStop(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "df['Lyrics'] = df['Lyrics'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "aljdoD5RSWAG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Lyrics'][2000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "CJrmkagoei-3",
        "outputId": "528a0a5f-349f-405c-9711-b66a48bb8002"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you used to be my closest ally (uh-huh) in this cold cold world of deception and lies oh we would defend and protect one another (one another) now i cant tell if were enemies or lovers so whos gonna rescue us from ourselves? when we gonna wake up baby? its time for lovin when we gonna wake up my baby? before its too late oh baby where did we go wrong baby? (uh-huh) did this cold cold world turn us into stone? well now all i battle is your ego and your pride its ticking like a time bomb ready to ignite hurtin me to fight so whos gonna rescue us from ourselves? when we gonna wake up baby? its time for lovin when we gonna wake up my baby? before its too late when we gonna wake up baby? its time for lovin when we gonna wake up my baby? before its too late when the smoke clears what will be left for us but tears and pain? why must we argue over the same things just to make up and go back again? its never too late it isnt too long cant get it right when no one thinks theyre wrong gotta get outta bed and take a look at whats goin on you might also like oh when we gonna wake up baby? its time for lovin when we gonna wake up my baby? before its too late when we gonna wake up baby? its time for lovin when we gonna wake up my baby? before its too late when we gonna wake up baby? its time for lovin when we gonna wake up my baby? before its too late oh baby (wake up) oh darlin (wake up) when we gonna wake up? (wake up) when we gonna take a look and see? (wake up) whats goin on? (wake up) oh whats goin on? (wake up) cause were wastin so much time (wake up) and were bout to lose it all (wake up) oh baby baby baby (wake up) said i need you baby oh i need my baby home bring my baby back to me (wake up) oh (wake up) oh baby (wake up)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "tokenizer = get_tokenizer('basic_english')"
      ],
      "metadata": {
        "id": "cjLzlJH2a712"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Factorize our labels\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(df.Genre)\n",
        "unique_genres = le.classes_\n",
        "print(unique_genres)\n",
        "num_genres = len(unique_genres)\n",
        "le.transform(df.Genre)\n",
        "\n",
        "mlb = preprocessing.MultiLabelBinarizer()\n",
        "labels = mlb.fit_transform(df[['Genre']].values)\n",
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km2d8XVPyhA2",
        "outputId": "ea15e537-50fc-47a9-da14-1fb22e8e0875"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Country' 'Pop' 'Rap' 'Rock']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       ...,\n",
              "       [1, 0, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [1, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create train, test, and validation sets\n",
        "\n",
        "TEST_SIZE = 0.2\n",
        "x, y = list(df['Lyrics']), labels\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = TEST_SIZE, stratify=y)\n",
        "#x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size = 0.5)\n",
        "\n",
        "print(f'Train Size: {len(x_train)}')\n",
        "print(f'Test Size: {len(x_test)}')\n",
        "#print(f'Test Size: {len(x_val)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NtI2tOvEK3p",
        "outputId": "c9e34c54-7313-466f-c2d9-86ffc471b66f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Size: 64228\n",
            "Test Size: 16057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12d8P4vSEQHs",
        "outputId": "fb207ebd-7bd1-478e-9546-2fda47ca6812"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       ...,\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a custom class for building the vocab, we decided not to use \n",
        "#it and instead use tortchtext method for vocabulary construction\n",
        "\n",
        "class Vocab():\n",
        "    \n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "        self.build_vocab()\n",
        "        \n",
        "    def build_vocab(self):\n",
        "        for sent in self.sentences:\n",
        "            #self.vocab.update(sent.split(' '))\n",
        "            self.vocab.update(tokenizer(sent))\n",
        "        \n",
        "        self.vocab = sorted(self.vocab)\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        \n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1 # 0 is the pad\n",
        "            \n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "            \n",
        "my_vocab = Vocab(x_train)"
      ],
      "metadata": {
        "id": "8yCB4J3Jk9v1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(my_vocab)) #with tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqdTcd0dl0HB",
        "outputId": "1382ac55-b98c-4f21-90b2-88a358efd239"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we construct the vocab using torchtext. The yield_tokens method \n",
        "#yields tokens instead of returning them so it works with an iterator\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "MIN_FREQ = 5\n",
        "\n",
        "def yield_tokens(lyrics):\n",
        "\n",
        "  for lyric in lyrics:\n",
        "\n",
        "    yield tokenizer(lyric)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(x_train), specials=[\"<unk>\"], min_freq = MIN_FREQ)\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "U8apHK8nlRiu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocab)) #with tokenizer\n",
        "vocab([')', 'hello', '<unk>', 'example', '?', 'cant', 'cannot', 'gonna', '*'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGTHPwpsmWDe",
        "outputId": "05e4c3a2-f898-4cad-f727-200262d934c4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43155\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12, 803, 0, 5060, 21, 60, 799, 90, 3336]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert tokens to indices of tokens in vocab\n",
        "\n",
        "index_train = []\n",
        "\n",
        "for lyrics in x_train:\n",
        "\n",
        "  indices = vocab(tokenizer(lyrics))\n",
        "  index_train.append(indices)\n",
        "\n",
        "index_test = []\n",
        "\n",
        "for lyrics in x_test:\n",
        "\n",
        "  indices = [vocab([token])[0] for token in tokenizer(lyrics)]\n",
        "  index_test.append(indices)"
      ],
      "metadata": {
        "id": "r_rTnIuzxM8V"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(index_train))\n",
        "print(len(index_test))"
      ],
      "metadata": {
        "id": "rbmMKTMrwW4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c0f694b-8ea2-4d8e-afd2-e055fa0ef672"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64228\n",
            "16057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize lyric lengths to determine good cutoff for padding\n",
        "\n",
        "lyricSize = []\n",
        "for lyrics in index_train + index_test:\n",
        "  \n",
        "  lyricSize.append(len(lyrics))\n",
        "\n",
        "print('Max: ', max(lyricSize))\n",
        "print('Min: ', min(lyricSize))\n",
        "lyricSize1 = sorted(lyricSize)[:math.floor(len(lyricSize)*0.998)]\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(6, 4))\n",
        "ax.hist(lyricSize1, 150)\n",
        " \n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "7mE4hcgJuGI9",
        "outputId": "9af81fe5-d07a-46ca-aad2-59ef8461186e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max:  13123\n",
            "Min:  0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function that adds padding of 0s to all the lyrics\n",
        "\n",
        "def addPadding(lyrics, max_len):\n",
        "\n",
        "    features = np.zeros((len(lyrics), max_len), dtype=int)\n",
        "\n",
        "    for i, lyric in enumerate(lyrics):\n",
        "\n",
        "      if len(lyric) > max_len:\n",
        "          features[i, :] = lyric[:max_len]\n",
        "        \n",
        "      else:\n",
        "        features[i, :len(lyric)] = lyric\n",
        "        \n",
        "    return features"
      ],
      "metadata": {
        "id": "IOKq4TRBzQxs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxWords = 950\n",
        "index_train_pad = addPadding(index_train, maxWords)\n",
        "index_test_pad = addPadding(index_test, maxWords)\n",
        "index_test_pad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXwUpMvfFAMS",
        "outputId": "c556d71d-aea2-4ee1-8cfd-42067799a350"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    6, 38472,    92, ...,     0,     0,     0],\n",
              "       [    2,   237,    15, ...,     0,     0,     0],\n",
              "       [   13,    64,  5512, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [   61,    50,   851, ...,     0,     0,     0],\n",
              "       [ 4810,   180,    97, ...,     0,     0,     0],\n",
              "       [   40,    40,     2, ...,     0,     0,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert input data and labels to tensors and then load into DataLoaders\n",
        "\n",
        "batchSize = 40\n",
        "\n",
        "data_train = TensorDataset(torch.from_numpy(index_train_pad).to(device), torch.from_numpy(y_train).to(device))\n",
        "data_val = TensorDataset(torch.from_numpy(index_test_pad).to(device), torch.from_numpy(y_test).to(device))\n",
        "\n",
        "train_loader = DataLoader(data_train, shuffle=True, batch_size=batchSize, drop_last=True)\n",
        "val_loader = DataLoader(data_val, shuffle=True, batch_size=batchSize, drop_last=True)"
      ],
      "metadata": {
        "id": "PSLeIAbRIW8s"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM Network\n",
        "class LSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units, batch_size, output_size, nLayers, bidirectional = False):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.batch = batch_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_units = hidden_units\n",
        "        self.output_size = output_size\n",
        "        \n",
        "  \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        #self.dropout = nn.Dropout(p=0.5)\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_units, num_layers= nLayers, bidirectional = bidirectional)\n",
        "\n",
        "        if bidirectional:\n",
        "\n",
        "          self.fc = nn.Linear(2*self.hidden_units, self.output_size)\n",
        "          self.nLayers = 2*nLayers\n",
        "\n",
        "        else:\n",
        "\n",
        "          self.fc = nn.Linear(self.hidden_units, self.output_size)\n",
        "          self.nLayers = nLayers\n",
        "        \n",
        "    def initialize_hidden_state(self, device):\n",
        "\n",
        "        h_0 = torch.randn(self.nLayers, self.batch, self.hidden_units).to(device)\n",
        "        c_0 = torch.randn(self.nLayers, self.batch, self.hidden_units).to(device)\n",
        "\n",
        "        #h_0 = torch.zeros((self.nLayers, self.batch, self.hidden_units)).to(device)\n",
        "        #c_0 = torch.zeros((self.nLayers, self.batch, self.hidden_units)).to(device)\n",
        "        return h_0, c_0\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        self.hidden = self.initialize_hidden_state(device=x.device)\n",
        "        output, self.hidden = self.lstm(x, self.hidden)\n",
        "        out = output[-1, :, :]\n",
        "        #out = self.dropout(out) #L2 reg does much better job than dropout to control overfitting here \n",
        "        out = self.fc(out)\n",
        "        out = nn.functional.softmax(out, dim=1)\n",
        "        \n",
        "        return out, self.hidden\n",
        "    "
      ],
      "metadata": {
        "id": "0rU5UnEgJz80"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiating model, loss function, optimizer\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "target_size = 4\n",
        "\n",
        "model_lstm = LSTM(vocab_size, embedding_dim = 150, hidden_units = 150,\n",
        "                  batch_size = batchSize, output_size = target_size, nLayers = 1, bidirectional = False)\n",
        "model_lstm.to(device)\n",
        "print(model_lstm)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "optimizer_lstm = torch.optim.Adam(model_lstm.parameters(), lr=2e-3, weight_decay=5e-8)\n",
        "#optimizer_lstm = torch.optim.Adam(model_lstm.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "id": "llSWGAd3M8TP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039e118a-cc6f-45f8-d82f-d633163fb57d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM(\n",
            "  (embedding): Embedding(43155, 150)\n",
            "  (lstm): LSTM(150, 150)\n",
            "  (fc): Linear(in_features=150, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "\n",
        "num_epochs = 20\n",
        " \n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "val_acc_list = []\n",
        "epoch_ct = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    epoch_ct += 1\n",
        "    print(\"Epoch: {:.3f}\".format(epoch_ct))\n",
        "    \n",
        "    model_lstm.train()\n",
        "    losses_train = []\n",
        "    \n",
        "    for inp, ground_truth in tqdm(train_loader):\n",
        "\n",
        "        predictions, _ = model_lstm(inp.permute(1, 0).to(device))\n",
        "      \n",
        "        truth = torch.max(ground_truth, 1)[1]\n",
        "        loss = loss_fn(predictions, truth) ## Calculate Loss\n",
        "        losses_train.append(loss.item())\n",
        "        \n",
        "        optimizer_lstm.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_lstm.step()\n",
        "\n",
        "\n",
        "    model_lstm.eval()\n",
        "    train_loss = torch.tensor(losses_train).mean()\n",
        "    print(\"Train Loss : {:.3f}\".format(train_loss))\n",
        "    train_loss_list.append(train_loss)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      losses_val = []\n",
        "\n",
        "      Y_truth, Y_preds = [],[]\n",
        "\n",
        "      for X, Y in val_loader:\n",
        "\n",
        "          preds, _ = model_lstm(X.permute(1, 0).to(device))\n",
        "          truth = torch.max(Y, 1)[1]\n",
        "\n",
        "          loss = loss_fn(preds, truth)\n",
        "          losses_val.append(loss.item())\n",
        "\n",
        "          Y_truth.append(truth)\n",
        "          Y_preds.append(preds.argmax(dim=-1))\n",
        "\n",
        "    val_loss = torch.tensor(losses_val).mean()\n",
        "    print(\"Validation Loss : {:.3f}\".format(val_loss))\n",
        "    val_loss_list.append(val_loss)\n",
        "\n",
        "    Y_truth = torch.cat(Y_truth)\n",
        "    Y_preds = torch.cat(Y_preds)\n",
        "    acc_score = accuracy_score(Y_truth.detach().cpu().numpy(), Y_preds.detach().cpu().numpy())\n",
        "    val_acc_list.append(acc_score)\n",
        "\n",
        "    print(\"Validation Acc  : {:.3f}\".format(acc_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPlcuP7BNhjQ",
        "outputId": "bf6bae4a-33e2-434b-8995-23ceb58e7838"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [01:59<00:00, 13.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.242\n",
            "Validation Loss : 1.170\n",
            "Validation Acc  : 0.568\n",
            "Epoch: 2.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [01:59<00:00, 13.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.153\n",
            "Validation Loss : 1.153\n",
            "Validation Acc  : 0.576\n",
            "Epoch: 3.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [02:00<00:00, 13.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.122\n",
            "Validation Loss : 1.128\n",
            "Validation Acc  : 0.609\n",
            "Epoch: 4.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [01:59<00:00, 13.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.093\n",
            "Validation Loss : 1.109\n",
            "Validation Acc  : 0.626\n",
            "Epoch: 5.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [01:59<00:00, 13.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.064\n",
            "Validation Loss : 1.088\n",
            "Validation Acc  : 0.649\n",
            "Epoch: 6.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [02:00<00:00, 13.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.044\n",
            "Validation Loss : 1.080\n",
            "Validation Acc  : 0.656\n",
            "Epoch: 7.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [01:59<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.028\n",
            "Validation Loss : 1.095\n",
            "Validation Acc  : 0.643\n",
            "Epoch: 8.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [01:59<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.013\n",
            "Validation Loss : 1.076\n",
            "Validation Acc  : 0.659\n",
            "Epoch: 9.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [01:59<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.998\n",
            "Validation Loss : 1.071\n",
            "Validation Acc  : 0.666\n",
            "Epoch: 10.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [01:59<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.989\n",
            "Validation Loss : 1.075\n",
            "Validation Acc  : 0.660\n",
            "Epoch: 11.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [01:59<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.980\n",
            "Validation Loss : 1.073\n",
            "Validation Acc  : 0.663\n",
            "Epoch: 12.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [02:00<00:00, 13.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.968\n",
            "Validation Loss : 1.071\n",
            "Validation Acc  : 0.665\n",
            "Epoch: 13.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [02:00<00:00, 13.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.959\n",
            "Validation Loss : 1.061\n",
            "Validation Acc  : 0.676\n",
            "Epoch: 14.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [02:00<00:00, 13.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.950\n",
            "Validation Loss : 1.064\n",
            "Validation Acc  : 0.672\n",
            "Epoch: 15.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [01:59<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.946\n",
            "Validation Loss : 1.061\n",
            "Validation Acc  : 0.676\n",
            "Epoch: 16.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [01:59<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.943\n",
            "Validation Loss : 1.067\n",
            "Validation Acc  : 0.671\n",
            "Epoch: 17.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [01:59<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.936\n",
            "Validation Loss : 1.062\n",
            "Validation Acc  : 0.677\n",
            "Epoch: 18.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [01:59<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.930\n",
            "Validation Loss : 1.067\n",
            "Validation Acc  : 0.671\n",
            "Epoch: 19.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [01:59<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.926\n",
            "Validation Loss : 1.062\n",
            "Validation Acc  : 0.677\n",
            "Epoch: 20.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1605/1605 [01:59<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.920\n",
            "Validation Loss : 1.065\n",
            "Validation Acc  : 0.673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "labels = Y_truth.detach().cpu().numpy()\n",
        "predictions = Y_preds.detach().cpu().numpy()\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(labels,\n",
        "                                                           predictions,\n",
        "                                                           average='macro')\n",
        "\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 score: {f1:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az5qXv2x6iqd",
        "outputId": "67da8a54-c2a0-4756-89b7-45538e0c03d7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.6766\n",
            "Recall: 0.6740\n",
            "F1 score: 0.6744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units, batch_size, output_size):\n",
        "        super(GRU, self).__init__()\n",
        "        self.batch = batch_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_units = hidden_units\n",
        "        self.output_size = output_size\n",
        "  \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        #self.dropout = nn.Dropout(p=0.5)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.hidden_units)\n",
        "        self.fc = nn.Linear(self.hidden_units, self.output_size)\n",
        "        \n",
        "    def initialize_hidden_state(self, device):\n",
        "\n",
        "        return torch.randn((1, self.batch, self.hidden_units)).to(device)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        self.hidden = self.initialize_hidden_state(x.device)\n",
        "        output, self.hidden = self.gru(x, self.hidden)\n",
        "        out = output[-1, :, :]\n",
        "        #out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "        out = nn.functional.softmax(out, dim=1)\n",
        "        \n",
        "        return out, self.hidden"
      ],
      "metadata": {
        "id": "QL_MZaUu6ivW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  \n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_units, batch_size, output_size):\n",
        "      super(RNN, self).__init__()\n",
        "      self.batch = batch_size\n",
        "      self.vocab_size = vocab_size\n",
        "      self.embedding_dim = embedding_dim\n",
        "      self.hidden_units = hidden_units\n",
        "      self.output_size = output_size\n",
        "\n",
        "      self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "      self.rnn = nn.RNN(self.embedding_dim, self.hidden_units)\n",
        "      self.fc = nn.Linear(self.hidden_units, self.output_size)\n",
        "      \n",
        "  def initialize_hidden_state(self, device):\n",
        "      return torch.randn((1, self.batch, self.hidden_units)).to(device)\n",
        "  \n",
        "  def forward(self, x):\n",
        "      x = self.embedding(x)\n",
        "      self.hidden = self.initialize_hidden_state(device=x.device)\n",
        "      output, self.hidden = self.rnn(x, self.hidden)\n",
        "      out = output[-1, :, :]\n",
        "      out = self.fc(out)\n",
        "      out = nn.functional.softmax(out, dim=1)\n",
        "      \n",
        "      return out, self.hidden"
      ],
      "metadata": {
        "id": "0zoi-0OnTt2m"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -Uq"
      ],
      "metadata": {
        "id": "0GNF8SMzfVz8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "SOL5DJkMfXJ6",
        "outputId": "83d88d67-7b20-4ecd-ccbc-5b85c3cb86ac"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I use Wandb to keep track of all my runs and do some hyperparameter tuning by playing around with the config."
      ],
      "metadata": {
        "id": "9jt4J6928PLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(\n",
        "    epochs=20,\n",
        "    embed_dim = 100,\n",
        "    hidden_dim = 100,\n",
        "    batch_size = 40,\n",
        "    learning_rate = 2e-3,\n",
        "    numLayers = 1,\n",
        "    weight_decay=8e-8,\n",
        "    bi = False,\n",
        "    min_freq = MIN_FREQ,\n",
        "    test_size = TEST_SIZE)"
      ],
      "metadata": {
        "id": "3aqQNJEDfl1s"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_loader(dataset, batch_size):\n",
        "\n",
        "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size, \n",
        "                                         shuffle=True,\n",
        "                                         drop_last=True)\n",
        "    return loader"
      ],
      "metadata": {
        "id": "kZJ5mbHYitD2"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make(config):\n",
        "    # Make the data\n",
        "    train_loader = make_loader(data_train, batch_size=config.batch_size)\n",
        "    test_loader = make_loader(data_val, batch_size=config.batch_size)\n",
        "\n",
        "    # Make the model\n",
        "   # model = GRU(vocab_size, embedding_dim = config.embed_dim,\n",
        "   #             hidden_units = config.hidden_dim, batch_size = config.batch_size, output_size = target_size)\n",
        "\n",
        "    #model = LSTM(vocab_size, embedding_dim = config.embed_dim, hidden_units = config.hidden_dim,\n",
        "    #              batch_size = config.batch_size, output_size = target_size, \n",
        "    #             nLayers = config.numLayers, bidirectional = config.bi).to(device)\n",
        "    model = RNN(vocab_size, embedding_dim = config.embed_dim,\n",
        "                    hidden_units = config.hidden_dim, \n",
        "                    batch_size = config.batch_size, output_size = target_size)\n",
        "    \n",
        "    model.to(device)\n",
        "\n",
        "    \n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "    \n",
        "    return model, train_loader, test_loader, criterion, optimizer"
      ],
      "metadata": {
        "id": "JBkK6qnUjUoN"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "truthG = 0\n",
        "predG = 0"
      ],
      "metadata": {
        "id": "zW_7BJzYJHdT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, criterion, optimizer, config, loader_v):\n",
        "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    # Run training and track with wandb\n",
        "    epoch_ct = 0\n",
        "\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "\n",
        "      epoch_ct += 1\n",
        "\n",
        "      print(\"Epoch: {:.3f}\".format(epoch_ct))\n",
        "      model.train()\n",
        "      losses_train = []\n",
        "      \n",
        "      for inp, ground_truth in loader:\n",
        "        \n",
        "        predictions, _ = model(inp.permute(1, 0).to(device))\n",
        "        \n",
        "      \n",
        "        truth = torch.max(ground_truth, 1)[1]\n",
        "        loss = criterion(predictions, truth) ## Calculate Loss\n",
        "        losses_train.append(loss.item())\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      model.eval()\n",
        "      train_loss = torch.tensor(losses_train).mean()\n",
        "      print(\"Train Loss : {:.3f}\".format(train_loss))\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "        losses_val = []\n",
        "\n",
        "        Y_truth, Y_preds = [],[]\n",
        "\n",
        "        for X, Y in loader_v:\n",
        "\n",
        "            preds, _ = model(X.permute(1, 0).to(device))\n",
        "            truth = torch.max(Y, 1)[1]\n",
        "\n",
        "            loss = criterion(preds, truth)\n",
        "            losses_val.append(loss.item())\n",
        "\n",
        "            Y_truth.append(truth)\n",
        "            Y_preds.append(preds.argmax(dim=-1))\n",
        "\n",
        "      val_loss = torch.tensor(losses_val).mean()\n",
        "      print(\"Validation Loss : {:.3f}\".format(val_loss))\n",
        "\n",
        "      Y_truth = torch.cat(Y_truth)\n",
        "      Y_preds = torch.cat(Y_preds)\n",
        "\n",
        "      truthG = Y_truth.detach().cpu().numpy()\n",
        "      predG = Y_preds.detach().cpu().numpy()\n",
        "      acc_val = accuracy_score(truthG, predG)\n",
        "\n",
        "      print(\"Validation Acc  : {:.3f}\".format(acc_val))\n",
        "\n",
        "      train_log(train_loss, val_loss, acc_val, epoch)\n",
        "\n",
        "\n",
        "\n",
        "def train_log(train_loss, val_loss, acc_val, epoch_ct):\n",
        "    # Where the magic happens\n",
        "    wandb.log({\"loss\": train_loss, \"val_loss\": val_loss, \"val_acc\": acc_val}, step=epoch_ct)"
      ],
      "metadata": {
        "id": "HzzYffa9i4P2"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "\n",
        "    # tell wandb to get started\n",
        "    with wandb.init(project=\"LSTM1\", config=hyperparameters):\n",
        "      # access all HPs through wandb.config, so logging matches execution!\n",
        "      config = wandb.config\n",
        "\n",
        "      # make the model, data, and optimization problem\n",
        "      model, train_loader, test_loader, criterion, optimizer = make(config)\n",
        "      print(model)\n",
        "\n",
        "      # and use them to train the model\n",
        "      train(model, train_loader, criterion, optimizer, config, loader_v = test_loader)\n",
        "\n",
        "      # and test its final performance\n",
        "      #test(model, test_loader)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Yih-PTYsqDax"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_pipeline(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PL70iSJvw9Ik",
        "outputId": "3a724a16-4af1-4f5d-d81d-836ebf1960a4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230417_034700-hxttcomq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/danya/LSTM1/runs/hxttcomq' target=\"_blank\">restful-water-22</a></strong> to <a href='https://wandb.ai/danya/LSTM1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/danya/LSTM1' target=\"_blank\">https://wandb.ai/danya/LSTM1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/danya/LSTM1/runs/hxttcomq' target=\"_blank\">https://wandb.ai/danya/LSTM1/runs/hxttcomq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (embedding): Embedding(43155, 100)\n",
            "  (rnn): RNN(100, 100)\n",
            "  (fc): Linear(in_features=100, out_features=4, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1.000\n",
            "test\n",
            "Train Loss : 1.381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|         | 1/20 [00:20<06:27, 20.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.377\n",
            "Validation Acc  : 0.283\n",
            "Epoch: 2.000\n",
            "test\n",
            "Train Loss : 1.377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|         | 2/20 [00:40<06:04, 20.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.376\n",
            "Validation Acc  : 0.286\n",
            "Epoch: 3.000\n",
            "test\n",
            "Train Loss : 1.377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|        | 3/20 [01:00<05:39, 19.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.379\n",
            "Validation Acc  : 0.283\n",
            "Epoch: 4.000\n",
            "test\n",
            "Train Loss : 1.376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|        | 4/20 [01:19<05:15, 19.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.375\n",
            "Validation Acc  : 0.286\n",
            "Epoch: 5.000\n",
            "test\n",
            "Train Loss : 1.377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|       | 5/20 [01:39<04:55, 19.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.382\n",
            "Validation Acc  : 0.286\n",
            "Epoch: 6.000\n",
            "test\n",
            "Train Loss : 1.376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|       | 6/20 [01:58<04:34, 19.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.377\n",
            "Validation Acc  : 0.283\n",
            "Epoch: 7.000\n",
            "test\n",
            "Train Loss : 1.377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|      | 7/20 [02:18<04:14, 19.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.377\n",
            "Validation Acc  : 0.286\n",
            "Epoch: 8.000\n",
            "test\n",
            "Train Loss : 1.379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|      | 8/20 [02:37<03:53, 19.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.384\n",
            "Validation Acc  : 0.283\n",
            "Epoch: 9.000\n",
            "test\n",
            "Train Loss : 1.377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|     | 9/20 [02:56<03:34, 19.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.377\n",
            "Validation Acc  : 0.283\n",
            "Epoch: 10.000\n",
            "test\n",
            "Train Loss : 1.377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|     | 10/20 [03:16<03:14, 19.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.380\n",
            "Validation Acc  : 0.239\n",
            "Epoch: 11.000\n",
            "test\n",
            "Train Loss : 1.377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|    | 11/20 [03:35<02:54, 19.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.379\n",
            "Validation Acc  : 0.286\n",
            "Epoch: 12.000\n",
            "test\n",
            "Train Loss : 1.376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|    | 12/20 [03:54<02:35, 19.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.374\n",
            "Validation Acc  : 0.286\n",
            "Epoch: 13.000\n",
            "test\n",
            "Train Loss : 1.376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|   | 13/20 [04:14<02:16, 19.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.374\n",
            "Validation Acc  : 0.283\n",
            "Epoch: 14.000\n",
            "test\n",
            "Train Loss : 1.376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|   | 14/20 [04:33<01:56, 19.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.374\n",
            "Validation Acc  : 0.286\n",
            "Epoch: 15.000\n",
            "test\n",
            "Train Loss : 1.376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|  | 15/20 [04:53<01:37, 19.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.373\n",
            "Validation Acc  : 0.286\n",
            "Epoch: 16.000\n",
            "test\n",
            "Train Loss : 1.377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|  | 16/20 [05:12<01:17, 19.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.376\n",
            "Validation Acc  : 0.283\n",
            "Epoch: 17.000\n",
            "test\n",
            "Train Loss : 1.377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%| | 17/20 [05:32<00:58, 19.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.376\n",
            "Validation Acc  : 0.286\n",
            "Epoch: 18.000\n",
            "test\n",
            "Train Loss : 1.377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%| | 18/20 [05:51<00:38, 19.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.383\n",
            "Validation Acc  : 0.286\n",
            "Epoch: 19.000\n",
            "test\n",
            "Train Loss : 1.379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|| 19/20 [06:10<00:19, 19.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.381\n",
            "Validation Acc  : 0.277\n",
            "Epoch: 20.000\n",
            "test\n",
            "Train Loss : 1.380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 20/20 [06:29<00:00, 19.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss : 1.381\n",
            "Validation Acc  : 0.285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>1.38013</td></tr><tr><td>val_acc</td><td>0.28529</td></tr><tr><td>val_loss</td><td>1.38103</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">restful-water-22</strong> at: <a href='https://wandb.ai/danya/LSTM1/runs/hxttcomq' target=\"_blank\">https://wandb.ai/danya/LSTM1/runs/hxttcomq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230417_034700-hxttcomq/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
